{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "EPOCHS = 25\n",
    "INIT_LR = 1e-3\n",
    "BS = 128\n",
    "directory_root = './data'\n",
    "width = 256\n",
    "height = 256\n",
    "depth = 3\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[INFO] Using device: {device}\")"
   ],
   "id": "ad22a39ef15bd29f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_paths = []\n",
    "label_names = []\n",
    "\n",
    "try:\n",
    "    print(\"[INFO] Scanning image directories ...\")\n",
    "    # Get plant folders, ignoring .DS_Store\n",
    "    root_dir = [d for d in os.listdir(directory_root) if d != \".DS_Store\"]\n",
    "\n",
    "    for plant_folder in root_dir:\n",
    "        plant_path = os.path.join(directory_root, plant_folder)\n",
    "        if not os.path.isdir(plant_path): continue\n",
    "\n",
    "        # Get disease folders\n",
    "        disease_folders = [d for d in os.listdir(plant_path) if d != \".DS_Store\"]\n",
    "\n",
    "        for disease_folder in disease_folders:\n",
    "            disease_path = os.path.join(plant_path, disease_folder)\n",
    "            if not os.path.isdir(disease_path): continue\n",
    "\n",
    "            print(f\"[INFO] Processing {disease_folder} ...\")\n",
    "\n",
    "            # Get images\n",
    "            images = [f for f in os.listdir(disease_path) if f != \".DS_Store\"]\n",
    "\n",
    "            for image_file in images:\n",
    "                file_path = os.path.join(disease_path, image_file)\n",
    "                # Check extensions\n",
    "                if file_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    image_paths.append(file_path)\n",
    "                    label_names.append(disease_folder)\n",
    "\n",
    "    print(f\"[INFO] Found {len(image_paths)} images.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error : {e}\")"
   ],
   "id": "41c4f7bc9c52a2b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create Label Mappings\n",
    "# Convert string labels (e.g., 'Tomato_Blight') to integers (0, 1, 2...)\n",
    "unique_labels = sorted(list(set(label_names)))\n",
    "class_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "n_classes = len(unique_labels)\n",
    "print(f\"[INFO] Classes: {unique_labels}\")\n",
    "\n",
    "# Convert all string labels to integers for the dataset\n",
    "label_indices = [class_to_idx[name] for name in label_names]"
   ],
   "id": "d7e8c8fa3aae951c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split Data\n",
    "# We split the paths, not the images. This is very fast and low memory.\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "    image_paths, label_indices, test_size=0.2, random_state=42, stratify=label_indices\n",
    ")"
   ],
   "id": "a8af090d253b2af4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class PlantDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transforms=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image ON DEMAND (Lazy Loading)\n",
    "        image_path = self.image_paths[idx]\n",
    "\n",
    "        # Open using PIL (Native RGB support, unlike OpenCV)\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {image_path}: {e}\")\n",
    "            # Return a black image in case of error to prevent crash\n",
    "            image = Image.new('RGB', (256, 256))\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        # Return label as a long tensor (required for CrossEntropyLoss)\n",
    "        return image, torch.tensor(self.labels[idx], dtype=torch.long)"
   ],
   "id": "f7131d87cf860161",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define Transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomRotation(25),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), shear=0.2, scale=(0.8, 1.2)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = PlantDataset(train_paths, train_labels, transforms=train_transforms)\n",
    "test_dataset = PlantDataset(test_paths, test_labels, transforms=test_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BS, shuffle=False, num_workers=4)"
   ],
   "id": "be0122db2244fd9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Model Definition\n",
    "class PlantDiseaseCNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(PlantDiseaseCNN, self).__init__()\n",
    "\n",
    "        # Dim calculations:\n",
    "        # Input: 256 x 256 x 3\n",
    "\n",
    "        # Block 1\n",
    "        # Conv 32 (3x3, padding=1) -> 256 x 256\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        # MaxPool (2x2) -> 256 / 2 = 128\n",
    "        # Output: 32 x 128 x 128\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "\n",
    "        # Block 2\n",
    "        # Conv 64 (3x3, padding=1) -> 128 x 128\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Block 3\n",
    "        # Conv 64 (3x3, padding=1) -> 128 x 128\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        # MaxPool (2x2) -> 128 / 2 = 64\n",
    "        # Output: 64 x 64 x 64\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "\n",
    "        # Block 4\n",
    "        # Conv 128 (3x3, padding=1) -> 64 x 64\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Block 5\n",
    "        # Conv 128 (3x3, padding=1) -> 64 x 64\n",
    "        self.conv5 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        # MaxPool (2x2) -> 64 / 2 = 32\n",
    "        # Output: 128 x 32 x 32\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout3 = nn.Dropout(0.25)\n",
    "\n",
    "        # Flatten -> Dense\n",
    "        # Feature map size: 128 channels * 32 * 32 = 131,072\n",
    "        self.fc1 = nn.Linear(128 * 32 * 32, 1024)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(1024)\n",
    "        self.dropout_fc = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(1024, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Block 1: Conv -> BN -> ReLU -> Pool -> Dropout\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Block 2 & 3: (Conv -> BN -> ReLU) x 2 -> Pool -> Dropout\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Block 4 & 5: (Conv -> BN -> ReLU) x 2 -> Pool -> Dropout\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # Dense: FC -> BN -> ReLU -> Dropout -> FC\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn_fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ],
   "id": "207d3db80bc13d42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model = PlantDiseaseCNN(n_classes).to(device)",
   "id": "c6bef50e24600667",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Optimizer and Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=INIT_LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)"
   ],
   "id": "a96935ab2a9338b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"[INFO] training network...\")\n",
    "\n",
    "# Lists to store history\n",
    "history = {'acc': [], 'val_acc': [], 'loss': [], 'val_loss': []}\n",
    "\n",
    "# Best model tracking\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_epoch_loss = val_loss / len(test_dataset)\n",
    "    val_epoch_acc = val_correct / val_total\n",
    "\n",
    "    # Update scheduler\n",
    "    scheduler.step(val_epoch_loss)\n",
    "\n",
    "    # Save best model\n",
    "    if val_epoch_loss < best_val_loss:\n",
    "        best_val_loss = val_epoch_loss\n",
    "        torch.save(model.state_dict(), 'best_cnn_model.pth')\n",
    "        print(f\"[INFO] Best model saved with val_loss: {val_epoch_loss:.4f}\")\n",
    "\n",
    "    history['loss'].append(epoch_loss)\n",
    "    history['acc'].append(epoch_acc)\n",
    "    history['val_loss'].append(val_epoch_loss)\n",
    "    history['val_acc'].append(val_epoch_acc)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS} - \"\n",
    "          f\"loss: {epoch_loss:.4f} - acc: {epoch_acc:.4f} - \"\n",
    "          f\"val_loss: {val_epoch_loss:.4f} - val_acc: {val_epoch_acc:.4f} - \"\n",
    "          f\"time: {int(epoch_mins)}m {int(epoch_secs)}s\")"
   ],
   "id": "4764ae9763f31636",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "acc = history['acc']\n",
    "val_acc = history['val_acc']\n",
    "loss = history['loss']\n",
    "val_loss = history['val_loss']\n",
    "epochs_range = range(1, len(acc) + 1)\n",
    "\n",
    "# Train and validation accuracy\n",
    "plt.plot(epochs_range, acc, 'b', label='Training accuracy')\n",
    "plt.plot(epochs_range, val_acc, 'r', label='Validation accuracy')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "# Train and validation loss\n",
    "plt.plot(epochs_range, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs_range, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "df88887983d42e8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load best model for evaluation\n",
    "print(\"[INFO] Loading best model for final evaluation...\")\n",
    "model.load_state_dict(torch.load('best_cnn_model.pth'))\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {(correct / total) * 100:.2f}\")"
   ],
   "id": "190a5034f27aeecb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
